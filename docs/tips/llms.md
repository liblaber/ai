# LLMs

Learn how the AI behind liblab.ai works and how to get the best results.

## What is an LLM?

LLM stands for Large Language Model. liblab.ai uses advanced LLMs to understand your requests and generate working applications. These models have been trained on vast amounts of code, documentation, and text to learn patterns in software development.

Think of an LLM as a very sophisticated pattern recognition system. It doesn't "know" information in the way humans do, but it can predict what code or features should come next based on similar patterns it learned during training.

## How liblab.ai Processes Your Requests

When you describe what you want to build, here's what happens:

- Understanding: The AI breaks down your prompt into smaller pieces called tokens.
- Pattern matching: The AI finds examples of similar apps, features, and code structures.
- Generation: The AI predicts code, design, and functionality to build what you requested.
- Assembly: The AI combines these predictions into a complete, working application.

## Context and Memory

Context includes prompts, previous responses, existing code, and uploaded files.

The AI has a context windowâ€”a limit on how much it can consider at once. For long conversations or complex projects, it may lose track of earlier details. If it seems to forget, try:

- Reminding it of key requirements
- Starting a new project for major changes
- Breaking complex requests into smaller steps

## AI Limitations

- Knowledge cutoff: Might not know newest versions or best practices
- Hallucination: Can generate plausible but incorrect code or info
- Literal interpretation: Follows instructions exactly; unclear prompts lead to mismatches
- No real-world testing: Generates based on patterns, not by running apps

## Getting Better Results

- Be specific about your tech stack
- Provide examples and reference files
- Test as you build
- Break down complex requests
- Give context about your users
